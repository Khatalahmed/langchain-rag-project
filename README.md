# ğŸ¦œğŸ”— LangChain RAG Project

> Build powerful AI applications locally with LangChain + Ollama â€” No cloud, no API costs, complete privacy.

[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![LangChain](https://img.shields.io/badge/LangChain-0.4-1C3C3C?style=for-the-badge&logo=chainlink&logoColor=white)](https://langchain.com)
[![Ollama](https://img.shields.io/badge/Ollama-Local_LLM-7C3AED?style=for-the-badge&logo=llama&logoColor=white)](https://ollama.ai)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)

---

## ğŸŒŸ Highlights

ğŸ”’ **100% Local** â€” Your data never leaves your machine  
ğŸ’° **Zero Cost** â€” No API keys or cloud subscriptions needed  
âš¡ **Fast** â€” No network latency, instant responses  
ğŸ“š **Educational** â€” Learn RAG, memory, routing & more  

---

## ğŸ“‚ Project Overview

| Category | Files | Description |
|----------|-------|-------------|
| **ğŸš€ Getting Started** | [main.py](cci:7://file:///d:/langchain-lab/main.py:0:0-0:0) | Basic Ollama LLM test |
| **ğŸ”— Chains** | [chain.py](cci:7://file:///d:/langchain-lab/chain.py:0:0-0:0), [cook_pipeline.py](cci:7://file:///d:/langchain-lab/cook_pipeline.py:0:0-0:0) | Prompt chaining & LCEL pipelines |
| **ğŸ“„ Document Loaders** | [load_pdf.py](cci:7://file:///d:/langchain-lab/load_pdf.py:0:0-0:0), [load_pdf_ocr.py](cci:7://file:///d:/langchain-lab/load_pdf_ocr.py:0:0-0:0), [load_story.py](cci:7://file:///d:/langchain-lab/load_story.py:0:0-0:0), [load_web.py](cci:7://file:///d:/langchain-lab/load_web.py:0:0-0:0) | Load PDFs, text files & web pages |
| **ğŸ§  RAG Pipeline** | [pdf_ai.py](cci:7://file:///d:/langchain-lab/pdf_ai.py:0:0-0:0) | Complete Retrieval-Augmented Generation |
| **ğŸ’¬ Memory** | [memory_chat.py](cci:7://file:///d:/langchain-lab/memory_chat.py:0:0-0:0) | Chatbot with conversation history |
| **ğŸ¯ Routing** | [prompt_router.py](cci:7://file:///d:/langchain-lab/prompt_router.py:0:0-0:0), [router_with_memory.py](cci:7://file:///d:/langchain-lab/router_with_memory.py:0:0-0:0), [router_memory_confidence.py](cci:7://file:///d:/langchain-lab/router_memory_confidence.py:0:0-0:0) | Intent-based routing with confidence scoring |
| **ğŸ“Š Structured Output** | [structured_output.py](cci:7://file:///d:/langchain-lab/structured_output.py:0:0-0:0) | JSON output with self-repair mechanism |

---

## ğŸš€ Quick Start

### Prerequisites

| Requirement | Version | Installation |
|-------------|---------|--------------|
| Python | 3.10+ | [python.org](https://python.org) |
| Ollama | Latest | [ollama.ai](https://ollama.ai) |

### Step 1: Clone Repository

git clone https://github.com/Khatalahmed/langchain-rag-project.git
cd langchain-rag-project

### Step 2: Setup Environment

# Create virtual environment
python -m venv venv

# Activate (Windows)
.\venv\Scripts\activate

# Activate (Mac/Linux)
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

### Step 3: Setup Ollama

ollama pull llama3.2

### Step 4: Run Examples

# Test basic LLM connection
python main.py

# Run full RAG pipeline
python pdf_ai.py

---

## ğŸ—ï¸ Architecture

langchain-rag-project/
â”‚
â”œâ”€â”€ ğŸ“ data/                      # Sample input documents
â”‚   â”œâ”€â”€ large_text.pdf
â”‚   â””â”€â”€ story.txt
â”‚
â”œâ”€â”€ ğŸ“ docs/                      # Additional documents for RAG
â”‚
â”œâ”€â”€ ğŸ“ outputs/                   # Generated outputs
â”‚
â”œâ”€â”€ ğŸ Core Scripts
â”‚   â”œâ”€â”€ main.py                   # LLM connection test
â”‚   â”œâ”€â”€ chain.py                  # Prompt chaining
â”‚   â”œâ”€â”€ cook_pipeline.py          # LCEL pipeline demo
â”‚   â””â”€â”€ pdf_ai.py                 # ğŸ”¥ Complete RAG pipeline
â”‚
â”œâ”€â”€ ğŸ“„ Document Loaders
â”‚   â”œâ”€â”€ load_pdf.py               # PDF loader
â”‚   â”œâ”€â”€ load_pdf_ocr.py           # OCR-based PDF
â”‚   â”œâ”€â”€ load_story.py             # Text file loader
â”‚   â””â”€â”€ load_web.py               # Web scraper
â”‚
â”œâ”€â”€ ğŸ§  Advanced Features
â”‚   â”œâ”€â”€ memory_chat.py            # Conversation memory
â”‚   â”œâ”€â”€ prompt_router.py          # Intent routing
â”‚   â”œâ”€â”€ router_with_memory.py     # Router + memory
â”‚   â”œâ”€â”€ router_memory_confidence.py # Confidence scoring
â”‚   â””â”€â”€ structured_output.py      # JSON output
â”‚
â”œâ”€â”€ requirements.txt              # Dependencies
â””â”€â”€ README.md                     # You are here!

---

## ğŸ› ï¸ Tech Stack

| Technology | Purpose |
|------------|---------|
| **LangChain** | LLM application framework |
| **LangGraph** | Stateful agent workflows |
| **Ollama** | Local LLM runtime |
| **FAISS** | Vector similarity search |
| **Sentence Transformers** | Local text embeddings |
| **PyPDF / Tesseract** | Document processing |

---

## ğŸ“š Concepts Covered

### Core LangChain
- âœ… LLM Integration with Ollama
- âœ… Prompt Templates
- âœ… LCEL (LangChain Expression Language)
- âœ… Chain composition

### RAG Pipeline
- âœ… Document Loading (PDF, Text, Web)
- âœ… Text Splitting & Chunking
- âœ… Embeddings (Sentence Transformers)
- âœ… Vector Stores (FAISS)
- âœ… Retrieval & Generation

### Advanced Patterns
- âœ… Conversation Memory
- âœ… Intent-based Routing
- âœ… Confidence Scoring
- âœ… Structured Output Parsing
- âœ… Self-repair Mechanisms

---

## ğŸ’¡ Why This Stack?

| Feature | Benefit |
|---------|---------|
| ğŸ  **Ollama** | Run LLMs locally â€” Llama 3.2, Mistral, Phi & more |
| ğŸ” **Privacy First** | Sensitive data stays on your machine |
| ğŸ’¸ **Cost Effective** | No per-token charges or rate limits |
| ğŸ“ **Learning Friendly** | Experiment freely without API costs |
| âš¡ **Low Latency** | No network round-trips |

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:
- ğŸ› Report bugs
- ğŸ’¡ Suggest features
- ğŸ”€ Submit pull requests

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€” feel free to use, modify, and distribute.

---

## â­ Support

If you found this helpful, please give it a â­ star!

---

Built with â¤ï¸ while learning LangChain for real-world AI applications.
